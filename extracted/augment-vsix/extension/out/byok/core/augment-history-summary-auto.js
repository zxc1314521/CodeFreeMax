"use strict";

const { debug } = require("../infra/log");
const { normalizeString, normalizeRawToken } = require("../infra/util");
const { state } = require("../config/state");
const { openAiCompleteText } = require("../providers/openai");
const { openAiResponsesCompleteText } = require("../providers/openai-responses");
const { anthropicCompleteText } = require("../providers/anthropic");
const { geminiCompleteText } = require("../providers/gemini");
const { buildSystemPrompt, buildOpenAiMessages, buildOpenAiResponsesInput, buildAnthropicMessages, buildGeminiContents } = require("./augment-chat");
const shared = require("./augment-chat.shared");
const { buildAbridgedHistoryText, exchangeRequestNodes, exchangeResponseNodes } = require("./augment-history-summary-auto.abridged");
const { REQUEST_NODE_TOOL_RESULT, REQUEST_NODE_HISTORY_SUMMARY } = require("./augment-protocol");
const { asRecord, asArray, asString, pick, normalizeNodeType } = shared;

const HISTORY_SUMMARY_CACHE_KEY = "augment-byok.historySummaryCache.v1";
const HISTORY_SUMMARY_CACHE = new Map();
let historySummaryCacheLoaded = false;

function maybeLoadHistorySummaryCacheFromStorage() {
  if (historySummaryCacheLoaded) return true;
  const ctx = state && typeof state === "object" ? state.extensionContext : null;
  if (!ctx || !ctx.globalState || typeof ctx.globalState.get !== "function") return false;

  try {
    const raw = ctx.globalState.get(HISTORY_SUMMARY_CACHE_KEY);
    const root = raw && typeof raw === "object" && !Array.isArray(raw) ? raw : null;
    const entries =
      (root && root.entries && typeof root.entries === "object" && !Array.isArray(root.entries) ? root.entries : null) ||
      (root && typeof root === "object" ? root : null) ||
      null;
    if (entries) {
      for (const [cid, v] of Object.entries(entries)) {
        const convId = normalizeString(cid);
        const rec = v && typeof v === "object" && !Array.isArray(v) ? v : null;
        if (!convId || !rec) continue;
        const summaryText = asString(rec.summaryText ?? rec.summary_text);
        const summarizedUntilRequestId = asString(rec.summarizedUntilRequestId ?? rec.summarized_until_request_id);
        const summarizationRequestId = asString(rec.summarizationRequestId ?? rec.summarization_request_id);
        const updatedAtMs = Number(rec.updatedAtMs ?? rec.updated_at_ms) || 0;
        if (!summarizedUntilRequestId) continue;
        HISTORY_SUMMARY_CACHE.set(convId, { summaryText, summarizedUntilRequestId, summarizationRequestId, updatedAtMs });
      }
    }
    historySummaryCacheLoaded = true;
    debug(`historySummary cache loaded: entries=${HISTORY_SUMMARY_CACHE.size}`);
    return true;
  } catch (err) {
    debug(`historySummary cache load failed (ignored): ${err instanceof Error ? err.message : String(err)}`);
    return false;
  }
}

async function persistHistorySummaryCacheToStorage() {
  const ctx = state && typeof state === "object" ? state.extensionContext : null;
  if (!ctx || !ctx.globalState || typeof ctx.globalState.update !== "function") return false;

  const entries = {};
  for (const [cid, v] of HISTORY_SUMMARY_CACHE.entries()) {
    entries[cid] = {
      summaryText: asString(v?.summaryText),
      summarizedUntilRequestId: asString(v?.summarizedUntilRequestId),
      summarizationRequestId: asString(v?.summarizationRequestId),
      updatedAtMs: Number(v?.updatedAtMs) || 0
    };
  }
  try {
    await ctx.globalState.update(HISTORY_SUMMARY_CACHE_KEY, { version: 1, entries });
    return true;
  } catch (err) {
    debug(`historySummary cache persist failed (ignored): ${err instanceof Error ? err.message : String(err)}`);
    return false;
  }
}

function nowMs() {
  return Date.now();
}

function approxTokenCountFromByteLen(len) {
  const BYTES_PER_TOKEN = 4;
  return Math.ceil(Number(len) / BYTES_PER_TOKEN);
}

function estimateRequestExtraSizeChars(req) {
  const r = asRecord(req);
  return (
    asString(pick(r, ["prefix"])).length +
    asString(pick(r, ["selected_code", "selectedCode"])).length +
    asString(pick(r, ["suffix"])).length +
    asString(pick(r, ["diff"])).length
  );
}

function nodeIsToolResult(n) {
  if (normalizeNodeType(n) !== REQUEST_NODE_TOOL_RESULT) return false;
  const tr = pick(n, ["tool_result_node", "toolResultNode"]);
  return tr && typeof tr === "object" && !Array.isArray(tr);
}

function hasHistorySummaryNode(nodes) {
  return asArray(nodes).some((n) => normalizeNodeType(n) === REQUEST_NODE_HISTORY_SUMMARY && pick(n, ["history_summary_node", "historySummaryNode"]) != null);
}

function historyContainsSummary(history) {
  return asArray(history).some((h) => {
    const it = asRecord(h);
    return hasHistorySummaryNode(it.request_nodes) || hasHistorySummaryNode(it.structured_request_nodes) || hasHistorySummaryNode(it.nodes);
  });
}

function requestContainsSummary(req) {
  const r = asRecord(req);
  const nodes = [...asArray(r.nodes), ...asArray(r.structured_request_nodes), ...asArray(r.request_nodes)];
  return hasHistorySummaryNode(nodes);
}

function exchangeHasToolResults(h) {
  return exchangeRequestNodes(h).some(nodeIsToolResult);
}

function estimateNodeSizeChars(node) {
  const n = asRecord(node);
  let out = 16;
  out += asString(pick(n, ["content"])).length;
  out += asString(pick(pick(n, ["text_node", "textNode"]), ["content"])).length;
  const tr = asRecord(pick(n, ["tool_result_node", "toolResultNode"]));
  if (normalizeNodeType(n) === REQUEST_NODE_TOOL_RESULT) {
    out += asString(pick(tr, ["tool_use_id", "toolUseId"])).length;
    out += asString(pick(tr, ["content"])).length;
    for (const c of asArray(pick(tr, ["content_nodes", "contentNodes"]))) {
      const cr = asRecord(c);
      out += 8;
      out += asString(pick(cr, ["text_content", "textContent"])).length;
      const img = asRecord(pick(cr, ["image_content", "imageContent"]));
      out += asString(pick(img, ["image_data", "imageData"])).length;
    }
  }
  const img = asRecord(pick(n, ["image_node", "imageNode"]));
  out += asString(pick(img, ["image_data", "imageData"])).length;
  for (const v of [pick(n, ["image_id_node", "imageIdNode"]), pick(n, ["ide_state_node", "ideStateNode"]), pick(n, ["edit_events_node", "editEventsNode"]), pick(n, ["checkpoint_ref_node", "checkpointRefNode"]), pick(n, ["change_personality_node", "changePersonalityNode"]), pick(n, ["file_node", "fileNode"]), pick(n, ["file_id_node", "fileIdNode"]), pick(n, ["history_summary_node", "historySummaryNode"])]) {
    if (v == null) continue;
    try { out += JSON.stringify(v).length; } catch {}
  }
  const tu = asRecord(pick(n, ["tool_use", "toolUse"]));
  out += asString(pick(tu, ["tool_use_id", "toolUseId"])).length;
  out += asString(pick(tu, ["tool_name", "toolName"])).length;
  out += asString(pick(tu, ["input_json", "inputJson"])).length;
  out += asString(pick(tu, ["mcp_server_name", "mcpServerName"])).length;
  out += asString(pick(tu, ["mcp_tool_name", "mcpToolName"])).length;
  const th = asRecord(pick(n, ["thinking", "thinking_node", "thinkingNode"]));
  out += asString(pick(th, ["summary"])).length;
  return out;
}

function estimateExchangeSizeChars(h) {
  const it = asRecord(h);
  const reqNodes = exchangeRequestNodes(it);
  const respNodes = exchangeResponseNodes(it);
  let n = 0;
  n += reqNodes.length ? reqNodes.map(estimateNodeSizeChars).reduce((a, b) => a + b, 0) : asString(it.request_message).length;
  n += respNodes.length ? respNodes.map(estimateNodeSizeChars).reduce((a, b) => a + b, 0) : asString(it.response_text).length;
  return n;
}

function estimateHistorySizeChars(history) {
  return asArray(history).map(estimateExchangeSizeChars).reduce((a, b) => a + b, 0);
}

function splitHistoryForSummary(history, tailSizeCharsToExclude, triggerOnHistorySizeChars, minTailExchanges) {
  const hs = asArray(history);
  if (!hs.length) return { head: [], tail: [] };
  const headRev = [];
  const tailRev = [];
  let seenChars = 0;
  let headChars = 0;
  let tailChars = 0;
  for (let i = hs.length - 1; i >= 0; i--) {
    const ex = hs[i];
    const sz = estimateExchangeSizeChars(ex);
    if (seenChars + sz < tailSizeCharsToExclude || tailRev.length < minTailExchanges) {
      tailRev.push(ex);
      tailChars += sz;
    } else {
      headRev.push(ex);
      headChars += sz;
    }
    seenChars += sz;
  }
  const totalChars = headChars + tailChars;
  if (totalChars < triggerOnHistorySizeChars) {
    const all = tailRev.concat(headRev).reverse();
    return { head: [], tail: all };
  }
  headRev.reverse();
  tailRev.reverse();
  return { head: headRev, tail: tailRev };
}

function adjustTailToAvoidToolResultOrphans(original, tailStart) {
  const hs = asArray(original);
  let start = Number.isFinite(Number(tailStart)) ? Math.floor(Number(tailStart)) : 0;
  while (start < hs.length) {
    if (!exchangeHasToolResults(hs[start])) break;
    if (start <= 0) break;
    start -= 1;
  }
  return start;
}

function resolveContextWindowTokens(hs, requestedModel) {
  const model = normalizeString(requestedModel);
  if (!model) return null;
  const overrides = asRecord(hs?.contextWindowTokensOverrides);
  const keys = Object.keys(overrides).sort((a, b) => String(b).length - String(a).length);
  for (const k of keys) {
    const key = normalizeString(k);
    const v = Number(overrides[k]);
    if (!key || !Number.isFinite(v) || v <= 0) continue;
    if (model.includes(key)) return Math.floor(v);
  }
  const d = Number(hs?.contextWindowTokensDefault);
  if (Number.isFinite(d) && d > 0) return Math.floor(d);
  return inferContextWindowTokensFromModelName(model);
}

function inferContextWindowTokensFromModelName(model) {
  const m = normalizeString(model).toLowerCase();
  if (!m) return null;
  if (m.includes("gemini-2.5-pro")) return 1000000;
  if (m.includes("claude-")) return 200000;
  if (m.includes("gpt-4o")) return 128000;
  const mk = m.match(/(?:^|[^0-9])([0-9]{1,4})k(?:\\b|[^0-9])/);
  if (mk && mk[1]) {
    const n = Number(mk[1]);
    if (Number.isFinite(n) && n > 0) {
      if (n === 128) return 128000;
      if (n === 200) return 200000;
      return n * 1024;
    }
  }
  return null;
}

function resolveHistorySummaryConfig(cfg) {
  const c = cfg && typeof cfg === "object" ? cfg : {};
  const hs = (c.historySummary && typeof c.historySummary === "object" && !Array.isArray(c.historySummary) ? c.historySummary : null) || (c.history_summary && typeof c.history_summary === "object" && !Array.isArray(c.history_summary) ? c.history_summary : null);
  if (!hs) return null;
  if (hs.enabled !== true) return null;
  const triggerOnHistorySizeChars = Number(hs.triggerOnHistorySizeChars);
  const maxTokens = Number(hs.maxTokens);
  const minTailExchanges = Number(hs.minTailExchanges);
  const tChars = Number.isFinite(triggerOnHistorySizeChars) && triggerOnHistorySizeChars > 0 ? Math.floor(triggerOnHistorySizeChars) : 0;
  if (!tChars) return null;
  return {
    ...hs,
    triggerOnHistorySizeChars: tChars,
    historyTailSizeCharsToExclude: Math.max(0, Math.floor(Number(hs.historyTailSizeCharsToExclude) || 0)),
    minTailExchanges: Number.isFinite(minTailExchanges) && minTailExchanges > 0 ? Math.floor(minTailExchanges) : 2,
    maxTokens: Number.isFinite(maxTokens) && maxTokens > 0 ? Math.floor(maxTokens) : 1024,
    timeoutSeconds: Math.max(1, Math.floor(Number(hs.timeoutSeconds) || 60)),
    cacheTtlMs: Math.max(0, Math.floor(Number(hs.cacheTtlMs) || 0)),
    maxSummarizationInputChars: Math.max(0, Math.floor(Number(hs.maxSummarizationInputChars) || 0)),
    triggerStrategy: normalizeString(hs.triggerStrategy) || "auto",
    triggerOnContextRatio: Number(hs.triggerOnContextRatio) || 0.7,
    targetContextRatio: Number(hs.targetContextRatio) || 0.55,
    prompt: typeof hs.prompt === "string" ? hs.prompt : "",
    abridgedHistoryParams: asRecord(hs.abridgedHistoryParams)
  };
}

function pickProviderById(cfg, providerId) {
  const pid = normalizeString(providerId);
  const list = Array.isArray(cfg?.providers) ? cfg.providers : [];
  if (!pid) return null;
  return list.find((p) => p && normalizeString(p.id) === pid) || null;
}

function normalizeProviderRequestDefaults(provider, maxTokens) {
  const base = provider && typeof provider === "object" && provider.requestDefaults && typeof provider.requestDefaults === "object" && !Array.isArray(provider.requestDefaults) ? provider.requestDefaults : {};
  const out = { ...base };
  const type = normalizeString(provider?.type);
  const mt = Number(maxTokens);
  const hasMt = Number.isFinite(mt) && mt > 0;
  if (hasMt) {
    const n = Math.floor(mt);
    if (type === "openai_responses") {
      out.max_output_tokens = n;
      if ("max_tokens" in out) delete out.max_tokens;
      if ("maxTokens" in out) delete out.maxTokens;
    } else if (type === "gemini_ai_studio") {
      const gc = out.generationConfig && typeof out.generationConfig === "object" && !Array.isArray(out.generationConfig) ? out.generationConfig : {};
      out.generationConfig = { ...gc, maxOutputTokens: n };
      if ("max_tokens" in out) delete out.max_tokens;
      if ("maxTokens" in out) delete out.maxTokens;
    } else {
      out.max_tokens = n;
    }
  }
  if (out.thinking) delete out.thinking;
  if (out.tools) delete out.tools;
  if (out.tool_choice) delete out.tool_choice;
  if (out.toolChoice) delete out.toolChoice;
  return out;
}

async function runSummaryModelOnce({ provider, model, prompt, chatHistory, maxTokens, timeoutMs, abortSignal }) {
  const p = provider && typeof provider === "object" ? provider : null;
  const type = normalizeString(p?.type);
  const baseUrl = normalizeString(p?.baseUrl);
  const apiKey = normalizeRawToken(p?.apiKey);
  const extraHeaders = p?.headers && typeof p.headers === "object" && !Array.isArray(p.headers) ? p.headers : {};
  const requestDefaults = normalizeProviderRequestDefaults(p, maxTokens);
  if (!type || !baseUrl || !normalizeString(model)) throw new Error("historySummary provider/model 未配置");
  if (!apiKey && Object.keys(extraHeaders).length === 0) throw new Error("historySummary provider 未配置 api_key（且 headers 为空）");
  if (!normalizeString(prompt) || !Array.isArray(chatHistory) || !chatHistory.length) throw new Error("historySummary prompt/chatHistory 为空");

  const augmentReq = {
    message: prompt,
    conversation_id: "",
    chat_history: chatHistory,
    tool_definitions: [],
    nodes: [],
    structured_request_nodes: [],
    request_nodes: [],
    agent_memories: "",
    mode: "",
    prefix: "",
    suffix: "",
    lang: "",
    path: "",
    user_guidelines: "",
    workspace_guidelines: "",
    rules: null,
    feature_detection_flags: {}
  };

  if (type === "openai_compatible") {
    return await openAiCompleteText({ baseUrl, apiKey, model, messages: buildOpenAiMessages(augmentReq), timeoutMs, abortSignal, extraHeaders, requestDefaults });
  }
  if (type === "anthropic") {
    return await anthropicCompleteText({ baseUrl, apiKey, model, system: buildSystemPrompt(augmentReq), messages: buildAnthropicMessages(augmentReq), timeoutMs, abortSignal, extraHeaders, requestDefaults });
  }
  if (type === "openai_responses") {
    const { instructions, input } = buildOpenAiResponsesInput(augmentReq);
    return await openAiResponsesCompleteText({ baseUrl, apiKey, model, instructions, input, timeoutMs, abortSignal, extraHeaders, requestDefaults });
  }
  if (type === "gemini_ai_studio") {
    const { systemInstruction, contents } = buildGeminiContents(augmentReq);
    return await geminiCompleteText({ baseUrl, apiKey, model, systemInstruction, contents, timeoutMs, abortSignal, extraHeaders, requestDefaults });
  }
  throw new Error(`historySummary 未知 provider.type: ${type}`);
}

function cacheGetFresh(conversationId, boundaryRequestId, now, ttlMs) {
  maybeLoadHistorySummaryCacheFromStorage();
  const cid = normalizeString(conversationId);
  const bid = normalizeString(boundaryRequestId);
  if (!cid || !bid) return null;
  const e = HISTORY_SUMMARY_CACHE.get(cid);
  if (!e) return null;
  if (ttlMs > 0 && now - Number(e.updatedAtMs || 0) > ttlMs) return null;
  if (normalizeString(e.summarizedUntilRequestId) !== bid) return null;
  return { summaryText: asString(e.summaryText), summarizationRequestId: asString(e.summarizationRequestId) };
}

function cacheGetFreshState(conversationId, now, ttlMs) {
  maybeLoadHistorySummaryCacheFromStorage();
  const cid = normalizeString(conversationId);
  if (!cid) return null;
  const e = HISTORY_SUMMARY_CACHE.get(cid);
  if (!e) return null;
  if (ttlMs > 0 && now - Number(e.updatedAtMs || 0) > ttlMs) return null;
  return { ...e };
}

async function cachePut(conversationId, boundaryRequestId, summaryText, summarizationRequestId, now) {
  maybeLoadHistorySummaryCacheFromStorage();
  const cid = normalizeString(conversationId);
  const bid = normalizeString(boundaryRequestId);
  if (!cid || !bid) return;
  HISTORY_SUMMARY_CACHE.set(cid, {
    summaryText: asString(summaryText),
    summarizedUntilRequestId: bid,
    summarizationRequestId: asString(summarizationRequestId),
    updatedAtMs: Number(now) || nowMs()
  });
  await persistHistorySummaryCacheToStorage();
}

async function deleteHistorySummaryCache(conversationId) {
  maybeLoadHistorySummaryCacheFromStorage();
  const cid = normalizeString(conversationId);
  if (!cid) return false;
  const existed = HISTORY_SUMMARY_CACHE.delete(cid);
  if (!existed) return false;
  await persistHistorySummaryCacheToStorage();
  return true;
}

async function clearHistorySummaryCacheAll() {
  maybeLoadHistorySummaryCacheFromStorage();
  const n = HISTORY_SUMMARY_CACHE.size;
  if (!n) return 0;
  HISTORY_SUMMARY_CACHE.clear();
  await persistHistorySummaryCacheToStorage();
  return n;
}

function computeTriggerDecision({ hs, requestedModel, totalWithExtra, convId }) {
  const triggerOnHistorySizeChars = Number(hs.triggerOnHistorySizeChars);
  const baseDecision = { thresholdChars: triggerOnHistorySizeChars, tailExcludeChars: hs.historyTailSizeCharsToExclude };
  const strategy = normalizeString(hs.triggerStrategy).toLowerCase();
  if (strategy === "chars") return totalWithExtra >= triggerOnHistorySizeChars ? baseDecision : null;

  const cwTokensRaw = resolveContextWindowTokens(hs, requestedModel);
  const cwTokens =
    strategy === "auto" && cwTokensRaw
      ? Math.min(cwTokensRaw, Math.max(0, Math.floor(triggerOnHistorySizeChars / 4)))
      : cwTokensRaw;
  if ((strategy === "ratio" || strategy === "auto") && cwTokens) {
    const approxTotalTokens = approxTokenCountFromByteLen(totalWithExtra);
    const ratio = cwTokens ? approxTotalTokens / cwTokens : 0;
    const triggerRatio = Number(hs.triggerOnContextRatio) || 0.7;
    if (ratio < triggerRatio) return null;
    const targetRatio = Number(hs.targetContextRatio) || 0.55;
    const thresholdTokens = Math.ceil(cwTokens * triggerRatio);
    const thresholdChars = thresholdTokens * 4;
    const targetTokens = Math.floor(cwTokens * targetRatio);
    const targetCharsBudget = targetTokens * 4;
    const summaryOverhead = (Number(hs.abridgedHistoryParams?.totalCharsLimit) || 0) + (Number(hs.maxTokens) || 0) * 4 + 4096;
    const tailExcludeChars = Math.max(0, targetCharsBudget - summaryOverhead);
    debug(`historySummary trigger ratio: conv=${convId} model=${normalizeString(requestedModel)} tokens≈${approxTotalTokens}/${cwTokens} ratio≈${ratio.toFixed(3)}`);
    return { thresholdChars, tailExcludeChars };
  }

  return totalWithExtra >= triggerOnHistorySizeChars ? baseDecision : null;
}

function computeTailSelection({ history, hs, decision }) {
  const split = splitHistoryForSummary(history, decision.tailExcludeChars, decision.thresholdChars, hs.minTailExchanges);
  if (!split.head.length || !split.tail.length) return null;
  const splitBoundaryRequestId = normalizeString(split.tail[0]?.request_id);
  if (!splitBoundaryRequestId) return null;
  let tailStart = history.findIndex((h) => normalizeString(h?.request_id) === splitBoundaryRequestId);
  if (tailStart < 0) tailStart = Math.max(0, history.length - split.tail.length);
  tailStart = adjustTailToAvoidToolResultOrphans(history, tailStart);
  const boundaryRequestId = normalizeString(history[tailStart]?.request_id);
  if (!boundaryRequestId) return null;
  const droppedHead = history.slice(0, tailStart);
  const tail = history.slice(tailStart);
  if (!droppedHead.length || !tail.length) return null;
  return { tailStart, boundaryRequestId, droppedHead, tail };
}

async function resolveSummaryText({ hs, cfg, convId, boundaryRequestId, history, tailStart, droppedHead, fallbackProvider, fallbackModel, timeoutMs, abortSignal }) {
  const now = nowMs();
  const cached = cacheGetFresh(convId, boundaryRequestId, now, hs.cacheTtlMs);
  if (cached) return { summaryText: cached.summaryText, summarizationRequestId: cached.summarizationRequestId, now };

  const provider = pickProviderById(cfg, hs.providerId) || fallbackProvider;
  const model = normalizeString(hs.model) || normalizeString(fallbackModel);
  let prompt = asString(hs.prompt);
  let inputHistory = droppedHead.slice();

  let usedRolling = false;
  if (hs.rollingSummary === true) {
    const prev = cacheGetFreshState(convId, now, hs.cacheTtlMs);
    if (prev && normalizeString(prev.summarizedUntilRequestId) && normalizeString(prev.summarizedUntilRequestId) !== boundaryRequestId) {
      const prevBoundaryPos = history.findIndex((h) => normalizeString(h?.request_id) === normalizeString(prev.summarizedUntilRequestId));
      if (prevBoundaryPos >= 0 && prevBoundaryPos < tailStart) {
        const delta = history.slice(prevBoundaryPos, tailStart);
        if (delta.length) {
          const prevExchange = { request_id: "byok_history_summary_prev", request_message: `[PREVIOUS_SUMMARY]\n${asString(prev.summaryText).trim()}\n[/PREVIOUS_SUMMARY]`, response_text: "", request_nodes: [], structured_request_nodes: [], nodes: [], response_nodes: [], structured_output_nodes: [] };
          inputHistory = [prevExchange, ...delta];
          usedRolling = true;
          prompt = `${normalizeString(hs.prompt)}\n\nYou will be given an existing summary and additional new conversation turns. Update the summary to include the new information. Output only the updated summary.`;
        }
      }
    }
  }

  const maxIn = Number(hs.maxSummarizationInputChars) || 0;
  if (maxIn > 0) {
    const shrink = () => estimateHistorySizeChars(inputHistory) > maxIn;
    if (usedRolling) while (inputHistory.length > 1 && shrink()) inputHistory.splice(1, 1);
    else while (inputHistory.length && shrink()) inputHistory.shift();
  }
  if (!inputHistory.length) return null;

  const timeout = Math.min(Math.max(1000, Number(timeoutMs) || 120000), hs.timeoutSeconds * 1000);
  const summaryText = normalizeString(await runSummaryModelOnce({ provider, model, prompt, chatHistory: inputHistory, maxTokens: hs.maxTokens, timeoutMs: timeout, abortSignal }));
  if (!summaryText) return null;
  const summarizationRequestId = `byok_history_summary_${now}`;
  await cachePut(convId, boundaryRequestId, summaryText, summarizationRequestId, now);
  return { summaryText, summarizationRequestId, now };
}

function buildHistoryEnd(tail) {
  return asArray(tail).map((h) => {
    const it = asRecord(h);
    return { request_id: it.request_id, request_message: it.request_message, response_text: it.response_text, request_nodes: exchangeRequestNodes(it), response_nodes: exchangeResponseNodes(it) };
  });
}

function injectHistorySummaryNodeIntoRequestNodes({ hs, req, tail, summaryText, summarizationRequestId, abridged }) {
  const template = asString(hs.summaryNodeRequestMessageTemplate);
  const historyEnd = buildHistoryEnd(tail);
  const summaryNode = { summary_text: summaryText, summarization_request_id: summarizationRequestId, history_beginning_dropped_num_exchanges: abridged.droppedBeginning, history_middle_abridged_text: abridged.text, history_end: historyEnd, message_template: template };
  const node = { id: 0, type: REQUEST_NODE_HISTORY_SUMMARY, content: "", history_summary_node: summaryNode };
  const r = req && typeof req === "object" ? req : null;
  if (!r) return null;
  if (!Array.isArray(r.request_nodes)) r.request_nodes = [];
  r.request_nodes.push(node);
  return node;
}

async function maybeSummarizeAndCompactAugmentChatRequest({ cfg, req, requestedModel, fallbackProvider, fallbackModel, timeoutMs, abortSignal }) {
  const hs = resolveHistorySummaryConfig(cfg);
  if (!hs) return false;
  const convId = normalizeString(req?.conversation_id);
  if (!convId) return false;
  const history = asArray(req?.chat_history);
  if (!history.length) return false;
  if (historyContainsSummary(history)) return false;
  if (requestContainsSummary(req)) return false;

  const totalChars = estimateHistorySizeChars(history);
  const totalWithExtra = totalChars + asString(req?.message).length + estimateRequestExtraSizeChars(req);
  const decision = computeTriggerDecision({ hs, requestedModel, totalWithExtra, convId });
  if (!decision) return false;

  const sel = computeTailSelection({ history, hs, decision });
  if (!sel) return false;

  const abridged = buildAbridgedHistoryText(history, hs.abridgedHistoryParams, sel.boundaryRequestId);
  const summary = await resolveSummaryText({ hs, cfg, convId, boundaryRequestId: sel.boundaryRequestId, history, tailStart: sel.tailStart, droppedHead: sel.droppedHead, fallbackProvider, fallbackModel, timeoutMs, abortSignal });
  if (!summary) return false;

  const injected = injectHistorySummaryNodeIntoRequestNodes({ hs, req, tail: sel.tail, summaryText: summary.summaryText, summarizationRequestId: summary.summarizationRequestId, abridged });
  if (!injected) return false;

  debug(`historySummary injected: conv=${convId} before≈${totalChars} tailStart=${sel.tailStart}`);
  return true;
}

module.exports = { maybeSummarizeAndCompactAugmentChatRequest, deleteHistorySummaryCache, clearHistorySummaryCacheAll };
